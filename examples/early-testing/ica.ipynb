{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scikit-learn -qq\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy import signal\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "# Create three sources\n",
    "s1 = np.sin(2 * time)\n",
    "s2 = np.cos(np.sin(3 * time))\n",
    "s3 = signal.sawtooth(2 * np.pi * time)\n",
    "\n",
    "# Stack the sources\n",
    "S = np.c_[s1, s2, s3]\n",
    "\n",
    "# Mix the sources\n",
    "A = np.array([[1, 1], [0.5, 2], [1.5, 1.0]])  # 3x2 Mixing matrix\n",
    "X = S.dot(A)  # Mixed signals\n",
    "\n",
    "# Apply ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Get the estimated sources\n",
    "A_ = ica.mixing_  # Get the estimated mixing matrix\n",
    "\n",
    "# Now, S_ contains the signals estimated to be the independent sources\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title(\"Original Sources\")\n",
    "plt.plot(S)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title(\"Mixed Signals\")\n",
    "plt.plot(X)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title(\"ICA Extracted Signals\")\n",
    "plt.plot(S_)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "# Create three sources\n",
    "s1 = np.sin(2 * time)\n",
    "s2 = np.sign(np.sin(3 * time))\n",
    "s3 = signal.sawtooth(2 * np.pi * time)\n",
    "\n",
    "# Stack the sources\n",
    "S = np.c_[s1, s2, s3]\n",
    "\n",
    "# Mix the sources into two observed dimensions\n",
    "A = np.array([[1, 1], [0.5, 2], [1.5, 1.0]])  # 3x2 Mixing matrix\n",
    "X = S.dot(A)  # Mixed 2D signals\n",
    "\n",
    "# Apply overcomplete ICA\n",
    "n_sources = 3\n",
    "weights, S_ = ica(X.T, n_sources, contrastfunction='tanh')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title(\"Original Sources\")\n",
    "plt.plot(S)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title(\"Mixed Signals\")\n",
    "plt.plot(X)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title(\"ICA Extracted Signals\")\n",
    "plt.plot(S_.T)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def whiten(X):\n",
    "    \"\"\"Whitening function for preprocessing\"\"\"\n",
    "    X_mean = torch.mean(X, dim=1, keepdim=True)\n",
    "    X = X - X_mean\n",
    "    cov = torch.mm(X, X.t()) / X.size(1)\n",
    "    U, S, V = torch.svd(cov)\n",
    "    X_white = torch.mm(U, torch.mm(torch.diag(1.0 / torch.sqrt(S)), torch.mm(U.t(), X)))\n",
    "    return X_white\n",
    "\n",
    "def whiten(X):\n",
    "    \"\"\"Whitening function that returns the whitened data, \n",
    "       the whitening matrix, and its inverse.\"\"\"\n",
    "    X_mean = torch.mean(X, dim=1, keepdim=True)\n",
    "    X_centered = X - X_mean\n",
    "    cov = torch.mm(X_centered, X_centered.t()) / X.size(1)\n",
    "    U, S, V = torch.svd(cov)\n",
    "    W_white = torch.mm(U, torch.mm(torch.diag(1.0 / torch.sqrt(S)), U.t()))\n",
    "    W_white_inv = torch.mm(U, torch.mm(torch.diag(torch.sqrt(S)), U.t()))\n",
    "    X_white = W_white @ X\n",
    "    return X_white, W_white, W_white_inv\n",
    "\n",
    "def overcomplete_ica(X, num_sources, max_iterations=1000, learning_rate=1e-3):\n",
    "    \"\"\"Overcomplete ICA using PyTorch\"\"\"\n",
    "    num_dimensions, num_samples = X.size()\n",
    "\n",
    "    # Whitening\n",
    "    X_white, W_white, W_white_inv = whiten(X)\n",
    "\n",
    "    # Initialize demixing matrix W with random values\n",
    "    W = torch.randn((num_sources, num_dimensions), requires_grad=True)\n",
    "    \n",
    "    optimizer = optim.Adam([W], lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\n",
    "\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Estimate sources\n",
    "        S = W @ X_white\n",
    "\n",
    "        # Using Negentropy as a measure of non-Gaussianity\n",
    "        # g(u) = log(cosh(u))\n",
    "        # g'(u) = tanh(u)\n",
    "        g = torch.log(torch.cosh(S))\n",
    "        g_prime = torch.tanh(S)\n",
    "        \n",
    "        loss = torch.sum(g) - torch.sum((W.t() @ g_prime))\n",
    "\n",
    "        # Backpropagate\n",
    "        (-loss).backward()\n",
    "        \n",
    "        # Update and normalize W\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            W += learning_rate * W.grad\n",
    "            W /= torch.norm(W, dim=1, keepdim=True)\n",
    "\n",
    "        # Display the loss\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss.item()}\")\n",
    "\n",
    "    return W, S, W_white, W_white_inv\n",
    "\n",
    "# Test\n",
    "num_samples = 2000\n",
    "num_dimensions = 2\n",
    "num_sources = 3  # > num_dimensions\n",
    "\n",
    "# Generate some toy data as mixed signals\n",
    "torch.manual_seed(0)\n",
    "S_true = torch.randn((num_sources, num_samples))\n",
    "\n",
    "\n",
    "# Example\n",
    "loc, scale = 0.0, 1.0  # the scale parameter, sometimes referred to as diversity\n",
    "S_true = torch.distributions.Laplace(loc, scale).rsample((num_sources, num_samples))\n",
    "S_true = torch.softmax(S_true, dim=0) * torch.norm(S_true, dim=0)\n",
    "\n",
    "#A = torch.randn((num_dimensions, num_sources))\n",
    "directions = torch.tensor([0, 2*torch.pi/3, 4*torch.pi/3])\n",
    "A_X = torch.sin(directions)\n",
    "A_Y = torch.cos(directions)\n",
    "A   = torch.stack([A_X, A_Y])\n",
    "X   = A @ S_true\n",
    "\n",
    "x_i, y_i = X[0], X[1]\n",
    "plt.scatter(x_i, y_i)\n",
    "\n",
    "X_white, W_white, W_white_inv = whiten(X)\n",
    "x_i, y_i = X_white[0], X_white[1]\n",
    "plt.scatter(x_i, y_i)\n",
    "print(torch.std(x_i), torch.std(y_i))\n",
    "\n",
    "if False:\n",
    "    # Extract sources\n",
    "    W_estimated, S_estimated, W_white, W_white_inv = overcomplete_ica(X, num_sources)\n",
    "    #print(W_estimated.shape, S_estimated.shape, W_white.shape, W_white_inv.shape)\n",
    "    X_reconstructed = W_white_inv @ W_estimated.T @ S_estimated\n",
    "\n",
    "    print(A)\n",
    "    print(W_estimated.T)\n",
    "\n",
    "    print(\"\\nreconstructed inputs:\")\n",
    "    print(X[:2, :3].numpy())\n",
    "    print(X_reconstructed[:2, :3].detach().numpy())\n",
    "\n",
    "    print(\"\\nreconstructed signals:\")\n",
    "    print(torch.nn.functional.layer_norm(S_true.T, (3,)).T[:3,:5].numpy())\n",
    "    print(S_estimated[:3, :5].detach().numpy())\n",
    "    \n",
    "if False:\n",
    "    from sklearn.decomposition import FastICA, KernelPCA\n",
    "\n",
    "    # Embed to 3D using Kernel PCA\n",
    "    kpca = KernelPCA(n_components=3, kernel=\"rbf\", fit_inverse_transform=True)\n",
    "    X_kpca = kpca.fit_transform(X_white.T.numpy())\n",
    "    print(\"KPCA shape\", X_kpca.shape)\n",
    "    \n",
    "    # ICA\n",
    "    ica = FastICA(n_components=3)\n",
    "    S_ica = ica.fit_transform(X_kpca)\n",
    "    A_ica = ica.mixing_\n",
    "    \n",
    "    A_2d = kpca.inverse_transform(A_ica)\n",
    "\n",
    "    print(\"ICA\", A_ica.T, A_2d)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # Original Data\n",
    "    plt.scatter(X_white[0], X_white[1], s=2, color='red', label='Original Data')\n",
    "\n",
    "    # ICA Directions\n",
    "    for comp in (A_2d*3):\n",
    "        plt.quiver(0, 0, comp[0], comp[1], angles='xy', scale_units='xy', scale=1, color='blue', label='ICA Directions')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('ICA on Data')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "if False:\n",
    "    def remove_projection(data, direction, threshold=0.5):\n",
    "        # Normalize the direction vector\n",
    "        direction_normalized = direction / np.linalg.norm(direction)\n",
    "        data_norms = np.linalg.norm(data, axis=-1)\n",
    "        data_normalized = data / data_norms[:, None]\n",
    "        \n",
    "        # Compute the projection of data onto the direction\n",
    "        dot_products = data_normalized @ direction_normalized\n",
    "        discounts    = data * np.clip(dot_products, a_min=0, a_max=1)\n",
    "        \n",
    "        return data - discounts\n",
    "\n",
    "    X_np = X_white.numpy().T\n",
    "\n",
    "    # First ICA run\n",
    "    ica1 = FastICA(n_components=1)\n",
    "    S_ica1 = ica1.fit_transform(X_np)\n",
    "    A_ica1 = ica1.mixing_\n",
    "\n",
    "    # Remove the projection onto the first ICA direction\n",
    "    X_np_removed = remove_projection(X_np, A_ica1)\n",
    "    \n",
    "    # Plotting...\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(X_np_removed[:, 0], X_np_removed[:, 1], s=2, color='red', label='Original Data')\n",
    "    plt.quiver(0, 0, A_ica1[0], A_ica1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='ICA Directions')\n",
    "\n",
    "    # Second ICA run on the modified data\n",
    "    ica2 = FastICA(n_components=1)\n",
    "    S_ica2 = ica2.fit_transform(X_np_removed)\n",
    "    A_ica2 = ica2.mixing_\n",
    "\n",
    "    # Now you have two directions from A_ica1 and A_ica2\n",
    "    # The third direction can be approximated by taking the orthogonal direction to these two.\n",
    "    #A_ica3 = np.cross(A_ica1.T, A_ica2.T)\n",
    "    \n",
    "    # Second ICA run on the modified data\n",
    "    ica3 = FastICA(n_components=1)\n",
    "    S_ica3 = ica3.fit_transform(X_np_removed)\n",
    "    A_ica3 = ica3.mixing_\n",
    "\n",
    "    # Plotting...\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # Original Data\n",
    "    plt.scatter(X_white[0], X_white[1], s=2, color='red', label='Original Data')\n",
    "\n",
    "    # ICA Directions\n",
    "    for comp in [A_ica1, A_ica2, A_ica3]:\n",
    "        plt.quiver(0, 0, comp[0], comp[1], angles='xy', scale_units='xy', scale=1, color='blue', label='ICA Directions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "# Test\n",
    "num_samples = 2000\n",
    "num_dimensions = 2\n",
    "num_sources = 3  # > num_dimensions\n",
    "\n",
    "# Generate some toy data as mixed signals\n",
    "torch.manual_seed(0)\n",
    "S_true = torch.randn((num_sources, num_samples))\n",
    "\n",
    "\n",
    "# Example\n",
    "loc, scale = 0.0, 1.0  # the scale parameter, sometimes referred to as diversity\n",
    "S_true = torch.distributions.Laplace(loc, scale).rsample((num_sources, num_samples))\n",
    "S_true = torch.softmax(S_true, dim=0) * torch.norm(S_true, dim=0)\n",
    "\n",
    "#A = torch.randn((num_dimensions, num_sources))\n",
    "directions = torch.tensor([0, 2*torch.pi/3, 4*torch.pi/3])\n",
    "A_X = torch.sin(directions)\n",
    "A_Y = torch.cos(directions)\n",
    "A   = torch.stack([A_X, A_Y])\n",
    "X   = A @ S_true\n",
    "\n",
    "x_i, y_i = X[0], X[1]\n",
    "plt.scatter(x_i, y_i)\n",
    "\n",
    "X_white, W_white, W_white_inv = whiten(X)\n",
    "x_i, y_i = X_white[0], X_white[1]\n",
    "plt.scatter(x_i, y_i)\n",
    "print(torch.std(x_i), torch.std(y_i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(SparseVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.pre_bias  = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.mid_bias  = nn.Parameter(torch.zeros(latent_dim))\n",
    "        self.post_bias = lambda : - self.pre_bias\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim, latent_dim, bias=False)\n",
    "        self.act_fn  = nn.ReLU()\n",
    "        self.decoder = lambda z : F.linear(z, self.encoder.weight.T)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mid   = self.encoder(x)\n",
    "        mid   = self.act_fn(mid)\n",
    "        recon = self.decoder(mid)\n",
    "        return recon, mid\n",
    "\n",
    "def loss_function(x, x_reconstructed, mid, kl_weight=5e-4):\n",
    "    MSE = F.mse_loss(x, x_reconstructed, reduction='sum')\n",
    "    L1  = torch.abs(mid).sum() * kl_weight\n",
    "    \n",
    "    return MSE + L1\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 2\n",
    "latent_dim = 3\n",
    "lr = 2e-3\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "# Data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(X_white.T)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model & Optimizer\n",
    "model = SparseVAE(input_dim, latent_dim)  # Move to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        data = batch[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data_recon, mid = model(data)\n",
    "        loss = loss_function(data, data_recon, mid)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {train_loss / len(dataloader.dataset)}\")\n",
    "print(f\"Epoch {epoch}: Loss = {train_loss / len(dataloader.dataset)}\")\n",
    "        \n",
    "model.eval()\n",
    "x = X_white.T[0]\n",
    "with torch.no_grad():\n",
    "    print(x, model(x))\n",
    "    \n",
    "plt.scatter(X_white[0], X_white[1])\n",
    "\n",
    "for (_x, _y) in model.encoder.weight.detach().numpy():\n",
    "    plt.quiver(0, 0, _x*2, _y*2, angles='xy', scale_units='xy', scale=1, color='yellow', label='ICA Directions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
