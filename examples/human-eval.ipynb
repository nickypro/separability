{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separability import Model\n",
    "from datasets import load_dataset\n",
    "\n",
    "m = Model(\"facebook/galactica-6.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from human_eval.data import write_jsonl\n",
    "from human_eval.evaluation import evaluate_functional_correctness\n",
    "\n",
    "def mktemp(name:str):\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "            suffix=name, delete=False\n",
    "        ) as temp_file:\n",
    "        temp_filename = temp_file.name\n",
    "    return temp_filename\n",
    "\n",
    "def gen_temp_jsonl_files():\n",
    "    return [mktemp(\"-problems.jsonl\"), mktemp(\"-samples.jsonl\")]\n",
    "\n",
    "def load_problems(n=None):\n",
    "    def __load_dataset():\n",
    "        _dataset = load_dataset(\"openai_humaneval\")[\"test\"]\n",
    "        if n is None:\n",
    "            return _dataset\n",
    "    \n",
    "        # Filter to only the first n problems\n",
    "        indices = list(range(0, n))\n",
    "        return _dataset.select(indices=indices)\n",
    "    \n",
    "    # Load problems in human-eval dict format\n",
    "    _dataset = __load_dataset()\n",
    "    return {d[\"task_id\"]: d for d in _dataset}\n",
    "\n",
    "\n",
    "def generate_one_completion(prompt):\n",
    "    [i,o] = m.generate(prompt, num=100, temperature=None)\n",
    "    #o = o.split(\"\\n\\ndef\")[0]\n",
    "    return o\n",
    " \n",
    "def generate_samples(problems):\n",
    "    num_samples_per_task = 1\n",
    "    samples = []\n",
    "    pbar = tqdm(desc=\"human-eval, gen\", total=num_samples_per_task*len(problems.keys()))\n",
    "    for _ in range(num_samples_per_task):\n",
    "        for task_id in problems:\n",
    "            samples.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"completion\": generate_one_completion(problems[task_id][\"prompt\"])\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return samples\n",
    "\n",
    "problems = load_problems(n=100)\n",
    "samples = generate_samples(problems)\n",
    "\n",
    "f_problems, f_samples = gen_temp_jsonl_files()\n",
    "write_jsonl(f_problems, list(problems.values()))\n",
    "write_jsonl(f_samples, samples)\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM=true\n",
    "from os import environ\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "out = evaluate_functional_correctness(\n",
    "    sample_file=f_samples,\n",
    "    problem_file=f_problems,\n",
    "    k = [1, 10],\n",
    ")\n",
    "\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "from accelerate import Accelerator\n",
    "from typing import Optional, List\n",
    "import json\n",
    "\n",
    "from lm_eval.evaluator import Evaluator as BigCodeEvaluator\n",
    "from lm_eval.tasks import ALL_TASKS as BIG_CODE_ALL_TASKS\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BigCodeEvalArgs:\n",
    "    model: str = \"codeparrot/codeparrot-small\"\n",
    "    peft_model: Optional[str] = None\n",
    "    revision: Optional[str] = None\n",
    "    use_auth_token: bool = False\n",
    "    trust_remote_code: bool = False\n",
    "    tasks: Optional[str] = None  # Assuming tasks will be a list; if not, adjust the type\n",
    "    instruction_tokens: Optional[str] = None\n",
    "    batch_size: int = 1\n",
    "    max_length_generation: int = 512\n",
    "    precision: str = \"fp32\"\n",
    "    load_in_8bit: bool = False\n",
    "    load_in_4bit: bool = False\n",
    "    limit: Optional[int] = None\n",
    "    postprocess: bool = True\n",
    "    allow_code_execution: bool = False\n",
    "    generation_only: bool = False\n",
    "    load_generations_path: Optional[str] = None\n",
    "    metric_output_path: str = \"evaluation_results.json\"\n",
    "    save_generations: bool = False\n",
    "    save_generations_path: str = \"generations.json\"\n",
    "    save_references: bool = False\n",
    "    \n",
    "    # New fields\n",
    "    model_ckpt: str = \"\"\n",
    "    prefix: str = \"\"\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.2\n",
    "    top_k: int = 0\n",
    "    top_p: float = 0.95\n",
    "    n_samples: int = 1\n",
    "    eos: str = \"\"\n",
    "    seed: int = 0\n",
    "\n",
    "def pattern_match(patterns, source_list):\n",
    "    \"\"\"Returns a list containing all values of the source_list that\n",
    "    match at least one of the patterns\"\"\"\n",
    "    task_names = set()\n",
    "    for pattern in patterns:\n",
    "        for matching in fnmatch.filter(source_list, pattern):\n",
    "            task_names.add(matching)\n",
    "    return list(task_names)\n",
    "\n",
    "def evaluate_bigcode(opt: Model, tasks:str = \"mbpp\"):\n",
    "    args = BigCodeEvalArgs(\n",
    "        model = opt.model_repo,\n",
    "        allow_code_execution = True,\n",
    "        tasks = tasks\n",
    "    )\n",
    "    \n",
    "    if args.tasks is None:\n",
    "        task_names = BIG_CODE_ALL_TASKS\n",
    "    else:\n",
    "        task_names = pattern_match(args.tasks.split(\",\"), BIG_CODE_ALL_TASKS)\n",
    "\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    results = {}\n",
    "    opt.tokenizer.eos_token = \"</s>\"\n",
    "    if not opt.tokenizer.eos_token:\n",
    "        if opt.tokenizer.bos_token:\n",
    "            opt.tokenizer.eos_token = opt.tokenizer.bos_token\n",
    "            print(\"bos_token used as eos_token\")\n",
    "        else:\n",
    "            raise ValueError(\"No eos_token or bos_token found\")\n",
    "    opt.tokenizer.pad_token = opt.tokenizer.eos_token\n",
    "\n",
    "    evaluator = BigCodeEvaluator(accelerator, opt.predictor, opt.tokenizer, args)\n",
    "\n",
    "    for task in task_names:\n",
    "        if args.generation_only:\n",
    "            if accelerator.is_main_process:\n",
    "                print(\"generation mode only\")\n",
    "            generations, references = evaluator.generate_text(task)\n",
    "            if accelerator.is_main_process:\n",
    "                with open(args.save_generations_path, \"w\") as fp:\n",
    "                    json.dump(generations, fp)\n",
    "                    print(f\"generations were saved at {args.save_generations_path}\")\n",
    "                if args.save_references:\n",
    "                    with open(\"references.json\", \"w\") as fp:\n",
    "                        json.dump(references, fp)\n",
    "                        print(\"references were saved\")\n",
    "        else:\n",
    "            results[task] = evaluator.evaluate(task)\n",
    "\n",
    "    results[\"config\"] = {\n",
    "        \"model\": args.model,\n",
    "        \"revision\": args.revision,\n",
    "        \"temperature\": args.temperature,\n",
    "        \"n_samples\": args.n_samples,\n",
    "    }\n",
    "    \n",
    "    dumped = json.dumps(results, indent=2)\n",
    "    if accelerator.is_main_process:\n",
    "        print(dumped)\n",
    "\n",
    "    with open(args.metric_output_path, \"w\") as f:\n",
    "        f.write(dumped)\n",
    "        \n",
    "evaluate_bigcode(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(m.tokenizer.all_special_tokens)\n",
    "print(m.tokenizer.all_special_tokens_extended)\n",
    "for i in range(0, 50):\n",
    "    print(i, m.tokenizer.convert_ids_to_tokens([i]))\n",
    "for i in range(m.cfg.d_vocab-100, m.cfg.d_vocab):\n",
    "    print(i, m.tokenizer.convert_ids_to_tokens([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
