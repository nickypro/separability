{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ccd7e32",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "We look at neuron activation distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3357e8d0",
   "metadata": {},
   "source": [
    "## Get Activations\n",
    "We first import dependancies and run the model to get some neuron distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52081088",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # if in google colab, download necessary python files\n",
    "  import google.colab \n",
    "  ! pip install -qq separability\n",
    "except ModuleNotFoundError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430583f1-9dfe-4cb0-acbd-bffd8cc4124b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from separability import Model\n",
    "from separability.activations import get_midlayer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec434dd-3c2e-4f9a-851c-f58c59203f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Model('nickypro/tinyllama-15m', limit=1000, dtype=\"fp32\")\n",
    "dataset = 'stories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40b996-bd78-4598-bf29-923140701433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_midlayer_activations( opt, dataset, 1e4, collect_ff=True, collect_attn=True )\n",
    "\n",
    "ff_activations   = data.raw[\"ff\"].permute( (1,2,0) )\n",
    "attn_activations = data.raw[\"attn\"].permute( (1,2,3,0) ).reshape( (opt.cfg.n_layers, opt.cfg.d_model, -1) ).clone()\n",
    "print( ff_activations.size() )\n",
    "print( attn_activations.size() )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b47ece08",
   "metadata": {},
   "source": [
    "## Plot Distributions for Neurons\n",
    "We can investigate the distribution of some random neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712d841-23d3-4bc8-94ae-5100f8e0d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_index(activations, layer, indices, ax=None, fill=False, n_bins=100):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for i in indices:\n",
    "        label = None if fill else f\"L{layer} Pos {i}\"\n",
    "        counts, bins = np.histogram( activations[layer][i].cpu().numpy(), bins=n_bins )\n",
    "        mids = (bins[:-1] + bins[1:]) / 2\n",
    "        if fill:\n",
    "            ax.semilogy( mids, counts, label=label, alpha=0.2, linewidth=0.5 )\n",
    "            ax.fill_between(mids, counts, color='skyblue', alpha=0.02)\n",
    "        else:\n",
    "            ax.semilogy( mids, counts, label=label, alpha=1, linewidth=1 )\n",
    "\n",
    "def plot_activation_indices(activations, indices):\n",
    "    for j in range(0, opt.n_layers, 7):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.title(f\"layer {j}\")\n",
    "        plot_layer_index(activations, j, indices)\n",
    "        plt.show()\n",
    "\n",
    "def plot_multiple(activations, layer, indices, labels, xlim, ylim, fill=False):\n",
    "    n_plots = len(activations)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(10, 4))\n",
    "    axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    axes[0].set_ylabel(f\"Unnormalized Probability Density\")\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_xlabel(f\"Neuron value in {labels[i]}\")\n",
    "        plot_layer_index(activations[i], layer, indices, ax, fill)\n",
    "        ax.semilogy([0, 0], ylim, \":k\", alpha=0.01)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        if not fill:\n",
    "            ax.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_activations = data.raw[\"attn\"].permute( (1,2,3,0) ).reshape( (opt.cfg.n_layers, opt.cfg.d_model, -1) ).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_activations.shape)\n",
    "zero_threshold = 1.0\n",
    "zero_ratio     = 1.0\n",
    "all_stds    = attn_activations.std(dim=-1)\n",
    "mean_stds   = all_stds.mean(dim=-1)\n",
    "means       = attn_activations.mean(dim=-1).unsqueeze(-1)\n",
    "__norm      = 1/mean_stds.unsqueeze(-1).unsqueeze(-1)\n",
    "n_zeros     = (attn_activations.abs()*__norm <  zero_threshold ).sum(dim=-1)\n",
    "n_non_zeros = (attn_activations.abs()*__norm >= zero_threshold ).sum(dim=-1)\n",
    "print(n_zeros.shape, n_non_zeros.shape)\n",
    "zeroness_score = n_zeros / n_non_zeros\n",
    "zeroed_activations = torch.ones_like(zeroness_score, dtype=bool) * (zeroness_score > zero_ratio)\n",
    "non_zeroed_activations = torch.logical_not(zeroed_activations)\n",
    "\n",
    "attn_zeroed     = attn_activations           *     zeroed_activations.unsqueeze(dim=-1)\n",
    "attn_not_zeroed = attn_activations           * non_zeroed_activations.unsqueeze(dim=-1)\n",
    "attn_re_zeroed  = (attn_activations - means) * non_zeroed_activations.unsqueeze(dim=-1)\n",
    "\n",
    "[n_layers, d_attn] = n_zeros.shape\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def find_range(xs, x_0):\n",
    "    l, r = x_0, x_0\n",
    "    while l > 0 and xs[l] > 0:\n",
    "        l -= 1\n",
    "    while r < len(xs) and xs[r] > 0:\n",
    "        r += 1\n",
    "    return l, r\n",
    "\n",
    "for l in range(n_layers):\n",
    "    std   = attn_activations[l].std(dim=-1).mean(dim=-1).item()\n",
    "    means = attn_activations[l].mean(dim=-1).unsqueeze(dim=-1)\n",
    "    print(f\"Layer {l+1} (std {std}):\")\n",
    "    plot_multiple([attn_zeroed, attn_not_zeroed, attn_re_zeroed],\n",
    "                l, range(d_attn), [\"zeroed\", \"not_zeroed\", \"meaned\"], [-0.5, 0.5], [0.9, 3e3], True) \n",
    "    plt.show()\n",
    "    \n",
    "    peak_threshold = 1.0\n",
    "    fig, ax = plt.subplots()\n",
    "    all_peaks = []\n",
    "    for n in range(d_attn):\n",
    "        acts = attn_activations[l,n]\n",
    "        #hist = torch.histc(acts, bins=100)\n",
    "        counts, bins = hist = np.histogram(acts.cpu().numpy(), bins=50 )\n",
    "        mids = ( bins[:-1] + bins[1:] )/2\n",
    "        peaks, properties = find_peaks(counts, height=200, distance=40)\n",
    "        peak_idx = peaks[np.argmin(np.abs(mids[peaks]))]\n",
    "        \n",
    "        close_acts = (counts*2 > counts[peak_idx])\n",
    "        min_idx, max_idx = find_range(close_acts, peak_idx)\n",
    "        if (max_idx - min_idx) > 20:\n",
    "            peak_idx = (min_idx + max_idx) // 2\n",
    "        \n",
    "        peak_pos = mids[peak_idx]\n",
    "        if len(peaks) > 1 or peak_pos/std > peak_threshold:\n",
    "            plot_layer_index([[acts-peak_pos]], 0, [0], ax=ax)\n",
    "            ax.plot(mids, close_acts, \":k\")\n",
    "            ax.plot(mids[peaks], counts[peaks], \"o\", label=f\"layer {l} peak {n}\")\n",
    "        all_peaks.append(mids[peak_idx])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot peaks based graphs\n",
    "    all_peaks = torch.tensor(np.array(all_peaks))\n",
    "    zero_peak_criteria  = all_peaks.abs().unsqueeze(dim=-1) / std < peak_threshold\n",
    "    attn_zero_peaks     = attn_activations[l] * zero_peak_criteria\n",
    "    attn_non_zero_peaks = attn_activations[l] * torch.logical_not(zero_peak_criteria)\n",
    "    attn_centered       = std * (attn_activations[l] - torch.tensor(all_peaks).unsqueeze(dim=-1)) / (all_stds[l].unsqueeze(dim=-1))\n",
    "    attn_mean_centered  = std * (attn_activations[l] - attn_activations[l].mean(dim=-1).unsqueeze(dim=-1)) / (all_stds[l].unsqueeze(dim=-1))\n",
    "    plot_multiple([[attn_zero_peaks], [attn_non_zero_peaks], [attn_centered], [attn_mean_centered]],\n",
    "        0, range(d_attn), [\"zeroed\", \"not_zeroed\", \"peak centered\", \"mean centered\"], [-0.25, 0.25], [0.9e1, 3e3], True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "caa55a89e6d7ad9e85de7c571769c816c820344d6fb9c860a740c7fc03f95f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
