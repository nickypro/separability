{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from separability import Model\n",
    "from separability.texts import prepare\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(\"facebook/galactica-125m\", 1000, dtype=\"fp16\")\n",
    "\n",
    "dataset, label, skip = prepare(\"pile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "i = 0\n",
    "for data in dataset:\n",
    "    text = data[label]\n",
    "    #print(i, text[:50])\n",
    "    inpt, attn, ff, outpt = m.get_text_activations(text, limit=1000)\n",
    "    activations.append(inpt)\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "print(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct batched dataset from activations List[Tensor]\n",
    "inputs = []\n",
    "for a in activations:\n",
    "    for b in a:\n",
    "        inputs.append(b)\n",
    "inputs = torch.stack(inputs).to(dtype=torch.float32)\n",
    "print(inputs.shape)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs)\n",
    "\n",
    "\n",
    "# Define basic torch mlp model\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Define the encoder\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, encoding_dim),\n",
    "        )\n",
    "\n",
    "        # Define the decoder\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoding_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def train_autoencoder(\n",
    "            dim,\n",
    "            loss_fn=None,\n",
    "            init_lr=1e-3,\n",
    "            n_epochs=50,\n",
    "            batch_size=64\n",
    "        ):\n",
    "    ae = AutoEncoder(input_dim=m.cfg.d_model, encoding_dim=dim)\n",
    "    ae = ae.to(device='cuda', dtype=torch.float32)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        #loss_fn = torch.nn.KLDivLoss()\n",
    "        #loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "    # start training\n",
    "    for epoch in (pbar := tqdm(range(n_epochs))):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            ae.parameters(), lr=init_lr/(epoch+1)\n",
    "        )\n",
    "        \n",
    "        for [batch] in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True): \n",
    "            # calculate loss\n",
    "            outputs = ae(batch)\n",
    "            loss = loss_fn(outputs, batch)\n",
    "            \n",
    "            # zero out old gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    for inpt in inputs[:1]: \n",
    "        output = ae(inpt)\n",
    "        print( inpt[:6].detach().cpu().to(torch.float16).numpy() )\n",
    "        print( output[:6].detach().cpu().to(torch.float16).numpy() )\n",
    "  \n",
    "for dim in [768, 512, 420, 360, 256, 128, 64, 32]: \n",
    "    train_autoencoder(dim) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
