{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from separability import Model\n",
    "m = Model(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_out = m.get_attn_pre_out_activations(\"Hello my name is <mask>\")\n",
    "print(pre_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Model(\"facebook/opt-125m\")\n",
    "print(opt.get_attn_pre_out_activations(\"Hello my name is\").shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_sample(m, \"We can see if this is working by using a sample text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separability.model import detached, pad_zeros\n",
    "from torch import Tensor\n",
    "\n",
    "ff_activations = {}\n",
    "\n",
    "def get_activation_of(name : str):\n",
    "    # Define hook function which adds output to self.activations\n",
    "    def hook(_model, _input, output):\n",
    "        ff_activations[name] = detached( _input )\n",
    "    return hook\n",
    "\n",
    "def register_activations(model: Model):\n",
    "    # register the forward hook\n",
    "    for layer_index, layer in enumerate(model.layers):\n",
    "        attn_out = layer[\"attn.out\"]\n",
    "        name = pad_zeros( layer_index ) + \"-attention-out\"\n",
    "        # print( f\"registering : ({name}), OPTAttention layer\" )\n",
    "        attn_out.register_forward_hook( get_activation_of( name ) )\n",
    "        continue\n",
    "    print( f\" - Registered {layer_index} Attention Out Layers\" )\n",
    "    \n",
    "def get_recent_activations():\n",
    "    \"\"\"\n",
    "    Returns a list of output tuples \\\n",
    "    ( \"##-attention-out\", output, attn_weights, key_values ) \\\n",
    "    from each attention block\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for key, value in ff_activations.items():\n",
    "        layer = []\n",
    "        layer.append( key )\n",
    "        for out in value:\n",
    "            if isinstance(out, Tensor):\n",
    "                layer.append( out )\n",
    "                continue\n",
    "\n",
    "            if out is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(out, (tuple, list)):\n",
    "                for o in out:\n",
    "                    layer.append( o )\n",
    "\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers\n",
    "\n",
    "register_activations( m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.get_residual_stream(text=\"the cat sat on the mat\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in act:\n",
    "    print(a[0])\n",
    "    print(a[1].shape)\n",
    "    print(a[2].shape)\n",
    "    print(a[1])\n",
    "    print(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained('roberta-base')\n",
    "print(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(\"This is an example of a <mask> model.\", return_tensors=\"pt\")\n",
    "\n",
    "    output = model.roberta(input_ids, output_hidden_states=False).last_hidden_state\n",
    "    print(output.shape)\n",
    "   \n",
    "    logits = model.lm_head(output) \n",
    "    print(logits.shape)\n",
    "    \n",
    "    output_ids = torch.argmax(logits, dim=-1)\n",
    "    print(tokenizer.batch_decode(output_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.roberta.encoder.layer[0].attention.self.query.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separability import Model\n",
    "from separability.eval import evaluate\n",
    "\n",
    "opt = Model(\"facebook/galactica-125m\", 1000)\n",
    "evaluate(opt, \"mmlu:all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separability import Model\n",
    "from separability.eval import evaluate\n",
    "m = Model(\"roberta-large\", 512)\n",
    "evaluate(m, \"mmlu:high_school_mathematics\", n_shot=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separability.prune import run_pruning\n",
    "from separability.data_classes import PruningConfig\n",
    "\n",
    "c=PruningConfig(**{\n",
    "    \"model_repo\": \"roberta-large\",\n",
    "    \"wandb_project\": \"testing-roberta\",\n",
    "    \"focus\": \"pile_codeless\",\n",
    "    \"cripple\": \"code\",\n",
    "    \"token_limit\": 512,\n",
    "    \"svd_attn\": False,\n",
    "    \"attn_mode\": \"pre-out\",\n",
    "    \"ff_scoring\": \"abs\",\n",
    "    \"attn_scoring\": \"abs\",\n",
    "    \"run_pre_test\": True,\n",
    "    \"collection_sample_size\": 1e4,\n",
    "    \"eval_sample_size\": 1e4,\n",
    "    \"ff_frac\": 0.05,\n",
    "    \"attn_frac\": 0.05,\n",
    "})\n",
    "\n",
    "run_pruning(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.layers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
