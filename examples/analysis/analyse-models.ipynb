{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# Just load seaborn & set theme and the chart looks better:\n",
    "! pip install seaborn -q\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.colors as cor\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "sns.set_theme()\n",
    "\n",
    "# set matplotlib dpi to 200 to make the images bigger\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "wandb_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_runs_pile_code = api.runs(\"seperability/seperability-pile-code\")\n",
    "_runs_pile_code_old = api.runs(\"seperability/seperability-pile-code\")\n",
    "_runs_pile_code_new = api.runs(\"seperability/new-method-compare\")\n",
    "_runs_pile_code = [ *_runs_pile_code_old, *_runs_pile_code_new ]\n",
    "_runs_code_python = api.runs(\"seperability/seperability-code-python\")\n",
    "_runs_code_python_new = api.runs(\"seperability/new-code-python\")\n",
    "#_runs_attn = api.runs(\"seperability/pile-code-attn\")\n",
    "use_fmt_map = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_name_number_unit(s):\n",
    "    name = re.search(r'[a-zA-Z ]+', s)\n",
    "    name = name.group() if name else ''\n",
    "    \n",
    "    number = re.search(r'[+-]?\\d+(\\.\\d*)?|\\.\\d+', s)\n",
    "    number = float(number.group()) if number else float('inf')\n",
    "    \n",
    "    unit = re.findall(r'(?<=[\\d.])[a-zA-Z]+', s)\n",
    "    unit = unit[0] if unit else ''\n",
    "    unit_value = 0\n",
    "    if unit.lower() == 'm':\n",
    "        unit_value = 1\n",
    "    elif unit.lower() == 'b':\n",
    "        unit_value = 1000\n",
    "        \n",
    "    return (name, number * unit_value, s)\n",
    "\n",
    "\n",
    "print( extract_name_number_unit('model-1.2M') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_map = {\"base\": \"Top1\", \"topk\": \"Top10\", \"skip\": \"Skip50-Top1\", \"topk_skip\": \"Skip50-Top10\", \"loss\": \"Loss\", \"perplexity\": \"Perplexity\"}\n",
    "color_map  = {\"pile\": \"tab:orange\", \"code\": \"tab:blue\", \"python\": \"tab:green\"}\n",
    "dataset_map = {\"pile\": \"Pile\", \"pile_codeless\": \"Pile\", \"python\": \"Python\", \"code\": \"Code\"}\n",
    "fmt_map = {\n",
    "    \"opt-125m\": \":y\",\n",
    "    \"opt-1.3b\": \"--y\",\n",
    "    \"opt-6.7b\": \"-yo\",\n",
    "    \"galactica-125m\": \":C2\",\n",
    "    \"galactica-1.3b\": \"--C2\",\n",
    "    \"galactica-6.7b\": \"-C2s\",\n",
    "    \"pythia-160m\": \":C4\",\n",
    "    \"pythia-1.4b\": \"--C4\",\n",
    "    \"pythia-6.9b\": \"-C4^\",\n",
    "    \"roberta-large\": \"-rx\",\n",
    "    \"llama-2-7b-hf\": \"-C6\",\n",
    "}\n",
    "fmt_map_loss = {\n",
    "    \"opt\": \"-yo\",\n",
    "    \"galactica\": \"-C2s\",\n",
    "    \"pythia\": \"-C4^\",\n",
    "    \"roberta\": \"-rx\",\n",
    "    \"llama-2-7b\": \"-C6\",\n",
    "}\n",
    "use_fmt_map = False\n",
    "\n",
    "def df_append(df, item: dict):\n",
    "    new_data = pd.DataFrame({ k:[v] for k,v in item.items() })\n",
    "    df = pd.concat([ df, new_data ], ignore_index=True )\n",
    "    return df\n",
    "\n",
    "def is_loss_metric(metric_name):\n",
    "    if metric_name == \"perplexity\":\n",
    "        return True\n",
    "    if metric_name[-4:] == \"loss\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normed(h, dataset, key):\n",
    "    if key == \"perplexity\":\n",
    "        arr = np.array([ float(x) for x in h[f\"loss_data/{dataset}/loss\"]])\n",
    "        if np.isnan(arr[0]):\n",
    "            return np.ones_like(arr)\n",
    "        return np.exp(arr)/np.exp(arr)[0]\n",
    "        #return np.exp(h[f\"loss_data/{dataset}/loss\"])/np.exp(h[f\"loss_data/{dataset}/loss\"][0])\n",
    "    if is_loss_metric(key):\n",
    "        arr = np.array([ float(x) for x in h[f\"loss_data/{dataset}/{key}\"]])\n",
    "        if np.isnan(arr[0]):\n",
    "            return np.ones_like(arr)\n",
    "        return arr/arr[0]\n",
    "        #bottom = h[f\"loss_data/{dataset}/{key}\"][0]\n",
    "        #return h[f\"loss_data/{dataset}/{key}\"]/h[f\"loss_data/{dataset}/{key}\"][0]\n",
    "    return h[f\"accuracy/{dataset}/{key}\"]/h[f\"accuracy/{dataset}/{key}\"][0]\n",
    "\n",
    "def calculate_area(x, y):\n",
    "    return np.trapz(y, x)\n",
    "\n",
    "class WandbRunData:\n",
    "    def __init__(self, run_name):\n",
    "        self.run_name = run_name.split(\"https://wandb.ai/\")[-1]\n",
    "        \n",
    "        if self.run_name in wandb_cache:\n",
    "            self.run = api.run(self.run_name)\n",
    "        else:\n",
    "            self.run = api.run(self.run_name)\n",
    "            wandb_cache[run_name] = self.run\n",
    "        \n",
    "        self.history = self.run.history()\n",
    "        self.h = self.history\n",
    "        c = self.run.config\n",
    "        \n",
    "        self.model_name = c[\"model_size\"].split(\"/\")[-1].lower()\n",
    "        \n",
    "        max_frac = max( c[\"ff_frac\"], c[\"attn_frac\"] )\n",
    "        self.frac = self.history[\"_step\"] * max_frac\n",
    "        \n",
    "        self.cripple, self.focus = c[\"cripple\"], c[\"focus\"]\n",
    "        self.cripple_label = dataset_map[self.cripple]\n",
    "        self.focus_label   = dataset_map[self.focus]\n",
    "        \n",
    "        # Get \"unique\" names\n",
    "        self.name_set = self.focus_label + \" \" + self.cripple_label\n",
    "        self.name_set_model = self.name_set + \" \" + self.model_name\n",
    "        \n",
    "    def get_metric(self, metric, diff):\n",
    "        if is_loss_metric(metric):\n",
    "            return self.get_loss_metric(metric)\n",
    "        if diff:\n",
    "            return self.get_diag_metric(metric)\n",
    "        return self.get_d_metric(metric)\n",
    "    \n",
    "    def get_loss_metric(self, metric):\n",
    "        self.scale = 1\n",
    "        focus_perf     = normed(self.h, self.focus,   metric) * self.scale\n",
    "        cripple_perf   = normed(self.h, self.cripple, metric) * self.scale\n",
    "\n",
    "        d_focus, d_cripple = focus_perf, cripple_perf\n",
    "        area = calculate_area(focus_perf, cripple_perf)*2 / (self.scale**2)\n",
    "        return focus_perf, cripple_perf, area\n",
    "    \n",
    "    def get_diag_metric(self, metric):\n",
    "        self.scale = 100\n",
    "        focus_perf     = normed(self.h, self.focus,   metric) * self.scale\n",
    "        cripple_perf   = normed(self.h, self.cripple, metric) * self.scale\n",
    "        \n",
    "        d_cripple, d_focus = self.scale-cripple_perf, self.scale-focus_perf\n",
    "        area = ( calculate_area(d_focus, d_cripple) - self.scale**2/2 )/(self.scale**2)\n",
    "        return d_cripple, d_focus, area\n",
    "     \n",
    "    def get_d_metric(self, metric):\n",
    "        self.scale = 100\n",
    "        focus_perf     = normed(self.h, self.focus,   metric) * self.scale\n",
    "        cripple_perf   = normed(self.h, self.cripple, metric) * self.scale\n",
    "        \n",
    "        d_cripple, d_focus = self.scale-cripple_perf, self.scale-focus_perf\n",
    "        area = calculate_area(d_focus, d_cripple-d_focus)*2 / (self.scale**2)\n",
    "        return d_cripple, d_focus, area\n",
    "     \n",
    "    def get_max_diff(self, metric, reversed=False):\n",
    "        self.scale = 100\n",
    "        focus_perf     = normed(self.h, self.focus,   metric) * self.scale\n",
    "        cripple_perf   = normed(self.h, self.cripple, metric) * self.scale\n",
    "        \n",
    "        return self.calculate_max_diff(focus_perf, cripple_perf, reversed=reversed)\n",
    "    \n",
    "    def calculate_max_diff(self, x, y, reversed=False):\n",
    "        x, y = np.array(x), np.array(y)\n",
    "        diff = (y-x) if reversed else (x-y)\n",
    "        return diff.max() \n",
    "\n",
    "def plot_frac_pruned(run_obj, metric):\n",
    "    r = run_obj\n",
    "    focus_perf     = normed(r.h, r.focus,   metric)\n",
    "    cripple_perf   = normed(r.h, r.cripple, metric)\n",
    "    metric_name = metric_map[metric]\n",
    "\n",
    "    # Begin plotting\n",
    "    plt.figure()\n",
    "    plt.plot(r.frac, focus_perf,   label=r.focus_label, color=\"tab:orange\")\n",
    "    plt.plot(r.frac, cripple_perf, label=r.cripple_label, color=\"tab:blue\")\n",
    "    plt.fill_between(x, focus_perf, cripple_perf, color=\"tab:purple\", alpha=0.2)\n",
    "    \n",
    "    # Add details\n",
    "    plt.xlim(-0.01, 1)\n",
    "    plt.ylim(-0.01, None)\n",
    "    plt.xlabel(\"Fraction of Model Pruned\")\n",
    "    plt.ylabel(\"Fraction of Original Accuracy\")\n",
    "    plt.title(f\"{metric_name} Accuracy ({r.model_name})\")\n",
    "    plt.legend()\n",
    "    \n",
    "    cripple_area = calculate_area(r.frac, cripple_perf)\n",
    "    focus_area   = calculate_area(r.frac, focus_perf)\n",
    "    area_ratio = (focus_area-cripple_area)/focus_area\n",
    "    print(metric_name, \"%.3f\" % area_ratio)\n",
    "    \n",
    "    return area_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(run_obj, metric, diff=True, label=None):\n",
    "    if is_loss_metric(metric):\n",
    "        return plot_loss_metric(run_obj, metric, diff, label)\n",
    "    return plot_perf_metric(run_obj, metric, diff, label)\n",
    "    \n",
    "def plot_perf_metric(run_obj, metric, diff=True, label=None): \n",
    "    r = run_obj\n",
    "    d_cripple, d_focus, area = r.get_metric(metric, diff)\n",
    "    if label is None:\n",
    "        label = r.model_name\n",
    "   \n",
    "    # Plot differences in ability \n",
    "    #fmt = fmt_map[r.model_name] if use_fmt_map else []\n",
    "    fmt = fmt_map[r.model_name] if r.model_name in fmt_map else None\n",
    "    \n",
    "    metric_name, scale = metric_map[metric], r.scale\n",
    "    plt.figure(r.name_set+metric_name)\n",
    "    #plt.title(f\"{metric_name} Accuracy, {r.cripple_label} Cripple {r.focus_label} Focus\")\n",
    "    #plt.xlim(-scale*0.01, scale*1.01)\n",
    "    #plt.ylim(-0.01, 1.01)\n",
    "    plt.xlabel(f\"Drop in {r.focus_label} {metric_name} Accuracy (∆%)\")\n",
    "    \n",
    "    if diff:\n",
    "        plt.ylabel(f\"Differential Drop in {r.cripple_label} Performance (∆%)\")\n",
    "        if fmt:\n",
    "            plt.plot(d_focus, d_cripple-d_focus, fmt, label=label)\n",
    "        else:\n",
    "            plt.plot(d_focus, d_cripple-d_focus, label=label)\n",
    "        plt.plot([0, scale], [0,     0], color=\"darkgray\", linestyle=\"--\", alpha=0.2)\n",
    "        plt.plot([0, scale], [scale, 0], color=\"darkgray\", linestyle=\"--\", alpha=0.2)\n",
    "    else:\n",
    "        #plt.ylabel(f\"Drop in {r.cripple_label} Performance (%)\")\n",
    "        plt.ylabel(f\"Drop in {r.cripple_label} {metric_name} Accuracy (∆%)\")\n",
    "        if fmt:\n",
    "            plt.plot(d_focus, d_cripple, fmt, label=label, markersize=3)\n",
    "        else:\n",
    "            plt.plot(d_focus, d_cripple, label=label)\n",
    "        plt.plot([0, scale], [scale, scale], color=\"darkgray\", linestyle=\"--\", alpha=0.2)\n",
    "        plt.plot([0, scale], [0,     scale], color=\"darkgray\", linestyle=\"--\", alpha=0.2)\n",
    "        \n",
    "    plt.legend()\n",
    "    \n",
    "    return area\n",
    "\n",
    "def plot_loss_metric(run_obj, metric, diff=False, label=None):\n",
    "    r = run_obj\n",
    "    print(r.model_name)\n",
    "    d_cripple, d_focus, area = r.get_metric(metric, diff)\n",
    "    scale = min([ max([*d_cripple, *d_focus, 1]), 100 ])\n",
    "    x_scale = min([ max([*d_cripple, *d_focus, 1]), 20 ])\n",
    "    y_scale = min([ max([*d_cripple, *d_focus, 1]), 130 ])\n",
    "    if label is None:\n",
    "        label = r.model_name\n",
    "    if \"bert\" in label:\n",
    "        return\n",
    "   \n",
    "    # Plot differences in ability \n",
    "    metric_name = metric_map[metric]\n",
    "    fig = plt.figure(r.name_set+metric_name, figsize=(4, 4))\n",
    "    #plt.title(f\"{metric_name} Accuracy, {r.cripple_label} Cripple {r.focus_label} Focus\")\n",
    "   \n",
    "    #fmt = fmt_map[r.model_name] if use_fmt_map else None\n",
    "    fmt = fmt_map[r.model_name] if r.model_name in fmt_map else None\n",
    "    #if r.model_name not in [\"opt-6.7b\", \"galactica-6.7b\", \"pythia-6.7b\", \"roberta-large\"]:\n",
    "    #    return 0\n",
    "    \n",
    "    # Custom formatter function for y-axis\n",
    "    def times_formatter(x, pos):\n",
    "        return f'{x:.0f}x'    \n",
    "    plt.xlabel(f\"Increase in {r.cripple_label} {metric_name}\")\n",
    "    plt.ylabel(f\"Increase in {r.focus_label} {metric_name}\")\n",
    "    if fmt is not None:\n",
    "        plt.loglog(d_cripple, d_focus, fmt, label=label, base=2, markersize=3)\n",
    "    else:\n",
    "        plt.loglog(d_cripple, d_focus, label=label, base=2, markersize=3)\n",
    "    if scale == 100:\n",
    "        plt.loglog([1,100], [1,100], base=2, color=\"darkgray\", linestyle=\"--\", alpha=0.2)\n",
    "        plt.xlim([0.6, x_scale*1.01])\n",
    "        plt.ylim([0.6, y_scale*1.01])\n",
    "    ax = fig.gca()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(times_formatter))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(times_formatter))\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_metrics(run_names, diff=True, best_run_obj=None):\n",
    "    #metrics_to_plot = [\"base\", \"topk\", \"skip\", \"topk_skip\", \"loss\", \"perplexity\"] # metric_map.keys()\n",
    "    metrics_to_plot = [\"base\", \"perplexity\"] # metric_map.keys()\n",
    "    main_metric = \"base\"\n",
    "  \n",
    "    # GET RUN DATA\n",
    "    if best_run_obj is None: \n",
    "        best_run_obj = defaultdict(list)\n",
    "        df_all = pd.DataFrame()\n",
    "        for run_name in run_names:\n",
    "            run_obj  = WandbRunData(run_name)\n",
    "            run_type = run_obj.name_set_model\n",
    "            model_name = run_obj.run.config[\"model_size\"].split(\"/\")[-1]\n",
    "            #run_summary, run_areas = {\"name\": run_obj.run.name}, {}\n",
    "            run_summary, run_areas = {\"Model\": model_name}, {}\n",
    "            for metric in metrics_to_plot:\n",
    "                _, _, area = run_obj.get_metric(metric, diff)\n",
    "                run_areas[metric] = area\n",
    "                if not is_loss_metric(metric):\n",
    "                    run_summary[metric_map[metric]] = area\n",
    "                #run_summary[\"url\"] = run_obj.run.url\n",
    "            df_all = df_append( df_all, run_summary )\n",
    "            best_run_obj[run_type].append( (run_areas[main_metric], run_obj, run_summary) )\n",
    "\n",
    "    sort_key = lambda k: extract_name_number_unit(k[0]) \n",
    "   \n",
    "    # PLOT RUNS BY BEST METRIC \n",
    "    df = pd.DataFrame() \n",
    "    for run_type, run_list in sorted(best_run_obj.items(), key=sort_key):\n",
    "        #print( f\"{run_type}, {run_list}\")\n",
    "        _, run_obj, run_summary = \\\n",
    "            run_list[ np.argmax([ a for (a,_b,_c) in run_list]) ]\n",
    "        for metric in metrics_to_plot:\n",
    "            area = plot_metric(run_obj, metric, diff)\n",
    "            #print(run_obj.model_name, \"%.3f\" % area)\n",
    "        df = df_append( df, run_summary )\n",
    "    \n",
    "    print(df.to_string(float_format=lambda x:\"%.3f\"%x))\n",
    "    plt.show()\n",
    "    \n",
    "    return best_run_obj\n",
    "    #print(df_all.to_string(float_format=lambda x:\"%.3f\"%x))\n",
    "   \n",
    "def plot_all_metrics(run_name_label, diff=False, run_list=None): \n",
    "    #print(\"model_name\", metric_map.values())\n",
    "    #metrics_to_plot = [\"base\", \"topk\", \"skip\", \"topk_skip\", \"loss\", \"perplexity\"] # metric_map.keys()\n",
    "    metrics_to_plot = [\"base\", \"perplexity\"] # metric_map.keys()\n",
    "   \n",
    "    if run_list is None: \n",
    "        run_list = []\n",
    "        df_all = pd.DataFrame()\n",
    "        for (run_name, run_label) in run_name_label:\n",
    "            run_obj  = WandbRunData(run_name)\n",
    "            run_type = run_obj.name_set_model\n",
    "            model_name = run_obj.run.config[\"model_size\"].split(\"/\")[-1]\n",
    "            #run_summary, run_areas = {\"name\": run_obj.run.name}, {}\n",
    "            run_summary = {\"Model\": model_name, \"label\": run_label}\n",
    "            for metric in metrics_to_plot:\n",
    "                score = run_obj.get_max_diff(metric)\n",
    "                if not is_loss_metric(metric):\n",
    "                    run_summary[\"score\"+metric_map[metric]] = score\n",
    "                #run_summary[\"url\"] = run_obj.run.url\n",
    "            df_all = df_append( df_all, run_summary )\n",
    "            run_list.append( (run_obj, run_summary) )\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for (run_obj, run_summary) in run_list:\n",
    "        for metric in metrics_to_plot:\n",
    "            area = plot_metric(run_obj, metric, diff=False, label=run_summary[\"label\"])\n",
    "            #print(run_obj.model_name, \"%.3f\" % area)\n",
    "        df = df_append( df, run_summary )\n",
    "    \n",
    "    print(df.to_string(float_format=lambda x:\"%.3f\"%x))\n",
    "    plt.show()\n",
    "    \n",
    "    return run_list\n",
    "    #print(df_all.to_string(float_format=lambda x:\"%.3f\"%x))   \n",
    " \n",
    "def filter_crashed_runs(runs, run_limit=None):\n",
    "    runs_filtered = []\n",
    "    for run in runs:\n",
    "        if run_limit and len(runs_filtered) > run_limit:\n",
    "            break\n",
    "        if run.state == \"crashed\":\n",
    "            continue\n",
    "        if not \"_step\" in run.summary or run.summary[\"_step\"] < 1:\n",
    "            continue\n",
    "        if run.config[\"ff_frac\"] <= 0:\n",
    "            continue\n",
    "        runs_filtered.append(run.url)\n",
    "    return runs_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Iterative vs Non-Iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot specific runs for appendix\n",
    "runs_bias_offset = [\n",
    "   (\"https://wandb.ai/seperability/pile-code-attn/runs/9v07rssi\", \"no bias offset\"),\n",
    "   (\"https://wandb.ai/seperability/pile-code-attn/runs/ihe734z1\", \"bias mean offset\"),\n",
    "]\n",
    "runs_iter = [\n",
    "   #(\"https://wandb.ai/seperability/method-compare/runs/v8crnxp8\", \"iterative\"),\n",
    "   #(\"https://wandb.ai/seperability/method-compare/runs/91w9ssrz\", \"single step\"),\n",
    "   (\"https://wandb.ai/seperability/method-compare/runs/5svab3oy\", \"single step\"),\n",
    "   (\"https://wandb.ai/seperability/method-compare/runs/20tvinil\", \"iterative\"),\n",
    "]\n",
    "runs_value_preout = [\n",
    "   #(\"https://wandb.ai/seperability/method-compare/runs/12n1hg1e\", \"pre-out\"),\n",
    "   (\"https://wandb.ai/seperability/method-compare/runs/e2b2a7cv\", \"value\"),\n",
    "   (\"https://wandb.ai/seperability/pile-code-attn/runs/c4ypwu6k\", \"pre-out\")\n",
    "]\n",
    "runs_both = [\n",
    "   (\"https://wandb.ai/seperability/method-compare/runs/peepo846\", \"2% FF 0.5% Attn\"),\n",
    "   (\"https://wandb.ai/seperability/method-compare/runs/20tvinil\", \"2% FF Only\")\n",
    "]\n",
    "plot_all_metrics(runs_bias_offset)\n",
    "plot_all_metrics(runs_iter)\n",
    "plot_all_metrics(runs_value_preout)\n",
    "plot_all_metrics(runs_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot A B C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs_filtered_new = filter_crashed_runs(_runs_pile_code_new, run_limit=None)\n",
    "new_run_list = plot_best_metrics(runs_filtered_new, False, new_run_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_filtered = filter_crashed_runs(_runs_pile_code, run_limit=None)\n",
    "print(runs_filtered)\n",
    "plot_best_metrics(runs_filtered, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_filtered = filter_crashed_runs(_runs_code_python_new, run_limit=None)\n",
    "py_runs_list = plot_best_metrics(runs_filtered, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runs_filtered = filter_crashed_runs([*_runs_code_python, *_runs_code_python_new], run_limit=None)\n",
    "plot_best_metrics(runs_filtered, False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
