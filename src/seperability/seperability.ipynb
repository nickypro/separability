{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperability Insight into OPT models\n",
    "Tests to see if it is possible to remove coding ability from Meta OPT model for different scales.\n",
    "Current methods are:\n",
    "- look at activation frequency of MLP mid layers\n",
    "- Look at 'crossover threshold' of Attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # if in google colab, download necessary python files\n",
    "  import google.colab \n",
    "  ! git clone https://github.com/pesvut/opt-tools.git && mv ./opt-tools/src/*.py .\n",
    "except ModuleNotFoundError:\n",
    "  pass\n",
    "! pip3 install -qqr requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "from model import Model\n",
    "from texts import prepare\n",
    "from activations import prune_and_evaluate, evaluate_all\n",
    "from data_classes import RunDataHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure initial model and tests\n",
    "model_size, token_limit  = \"1.3b\", 1000\n",
    "run_pre_test             = True\n",
    "pre_removals = []\n",
    "\n",
    "# Removals parameters\n",
    "ff_frac,   ff_eps   = 0.02, 0.001\n",
    "attn_frac           = 0.01\n",
    "cripple, focus      = \"code\", \"pile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data logging\n",
    "wandb.init(project=\"seperability-code-pile\")\n",
    "c = wandb.config\n",
    "c.update({\n",
    "    \"model_size\"  : model_size,\n",
    "    \"token_limit\" : token_limit,\n",
    "    \"run_pre_test\": run_pre_test,\n",
    "    \"ff_frac\"  : ff_frac,\n",
    "    \"ff_eps\"   : ff_eps,\n",
    "    \"attn_frac\": attn_frac,\n",
    "    \"cripple\": cripple,\n",
    "    \"focus\"  : focus,\n",
    "})\n",
    "\n",
    "# Load model and show details about model\n",
    "history = RunDataHistory()\n",
    "opt = Model( c.model_size, limit=c.token_limit )\n",
    "\n",
    "# Pre-pruning of model\n",
    "opt.delete_ff_keys_from_files(pre_removals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model before removal of any neurons\n",
    "if c.run_pre_test:\n",
    "    history.add( evaluate_all( opt, 1e5 ) )\n",
    "    print( history.df.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do some pruning of the feed forward layers\n",
    "for i in range(3):\n",
    "    data = prune_and_evaluate( opt, c.ff_frac, c.attn_frac, c.ff_eps, cripple=c.cripple, focus=c.focus )\n",
    "    history.add( data )\n",
    "print(history.df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next do some pruning of the feed forward layers + attention layers\n",
    "for i in range(3):\n",
    "    data = prune_and_evaluate( opt, c.ff_frac, c.attn_frac, c.ff_eps, cripple=c.cripple, focus=c.focus )\n",
    "    history.add( data )\n",
    "print(history.df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.df.T.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "caa55a89e6d7ad9e85de7c571769c816c820344d6fb9c860a740c7fc03f95f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
