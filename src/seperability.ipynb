{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # if in google colab, download necessary python files\n",
    "  import google.colab\n",
    "  ! git clone https://github.com/pesvut/opt-tools.git && mv ./opt-tools/src/*.py .\n",
    "except ModuleNotFoundError:\n",
    "  pass\n",
    "! pip install -qq transformers datasets evaluate zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from texts import prepare_pile, prepare_code\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare( dataset_name ):\n",
    "    if dataset_name == 'pile':\n",
    "        return prepare_pile()\n",
    "    if dataset_name == 'code':\n",
    "        return prepare_code()\n",
    "\n",
    "opt = Model(\"125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( opt.model.decoder.layers[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def setup_counter(ff_keys):\n",
    "    shape = ff_keys.size()\n",
    "    counter = []\n",
    "    for i in range(shape[0]):\n",
    "        counter.append( torch.zeros( shape[-1]))\n",
    "    return torch.stack(counter).to( opt.device )\n",
    "\n",
    "def count_keys( dataset_name, limit=1000, sample_size=10000, num_samples=1, \n",
    "                check_accuracy=False, k=10, check_skips=False ):\n",
    "    dataset, label, skip_eval = prepare( dataset_name )\n",
    "    counters = []\n",
    "    counter = None\n",
    "    curr_count = 0\n",
    "    with tqdm(total=sample_size*num_samples) as pbar:\n",
    "        for data in dataset:\n",
    "            text = data[label]\n",
    "            input_ids = opt.get_ids( text, limit=limit )\n",
    "            ids = input_ids.squeeze().detach().cpu()\n",
    "\n",
    "            # Criteria for counting the token activation\n",
    "            criteria = torch.ones_like( ids, dtype=torch.bool )\n",
    "\n",
    "            # check if prediction is accurate enough to count\n",
    "            if check_accuracy:\n",
    "                residual_stream = opt.get_residual_stream( input_ids=input_ids )\n",
    "                logits = opt.unembed( residual_stream[-1] ).detach().cpu()\n",
    "                top_k_tokens = opt.top_k_tokens( logits, k=k ).squeeze()\n",
    "\n",
    "                for index in range(len(ids)-1):\n",
    "                    criteria[index] *= (ids[index+1] in top_k_tokens[index])\n",
    "\n",
    "            # Choose a set of token ids to skip \n",
    "            if check_skips:\n",
    "                skip_ids = set()\n",
    "                for skip_string in skip_eval:\n",
    "                    skip_id = int( opt.get_ids( skip_string ).squeeze()[-1] )\n",
    "                    skip_ids.add( skip_id )\n",
    "\n",
    "                for index in range(len(ids)-1):\n",
    "                    criteria[index] *= (ids[index+1] in skip_ids)\n",
    "                \n",
    "            num_valid_tokens = criteria.sum()\n",
    "            curr_count += num_valid_tokens\n",
    "\n",
    "            ff_keys = opt.get_ff_key_activations(input_ids=input_ids)\n",
    "            if counter is None:\n",
    "                counter = setup_counter(ff_keys)\n",
    "            \n",
    "            for layer_index, layer in enumerate(ff_keys):\n",
    "                for token_index, key_activation in enumerate(layer):\n",
    "                    if not criteria[token_index]:\n",
    "                        continue\n",
    "                    counter[layer_index] += ( key_activation != 0 )\n",
    "\n",
    "\n",
    "            pbar.update( int(num_valid_tokens) )\n",
    "            if curr_count > sample_size:\n",
    "                counter = counter / curr_count\n",
    "                counters.append( counter.detach().cpu() )\n",
    "                print( f'sample {len(counters)}: {curr_count}' )\n",
    "                \n",
    "                counter = setup_counter(ff_keys)\n",
    "                curr_count = 0\n",
    "            \n",
    "            if len( counters ) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    return torch.stack( counters )\n",
    "\n",
    "def acc_str( acc, pred ):\n",
    "    percentage = (100*round(acc/pred, 3))\n",
    "    return \"%.1f\"%percentage + \"% - ( {acc}/{pred} )\"\n",
    "\n",
    "def evaluate( dataset_name, limit : int = 1e6, topk: int = 1 ):\n",
    "    dataset, label, skip_eval = prepare( dataset_name )\n",
    "    out = opt.evaluate_dataset( dataset, token_limit=1000, k=topk,\n",
    "        start_index=1, stopping_index=limit, skip_eval=skip_eval,\n",
    "        dataset_text_label=label, count_tokens=False )\n",
    "    print( f'{dataset_name} w/ skip:', \n",
    "        acc_str(out['num_skip_accurate'], out['num_skip_predictions']) )\n",
    "    print( f'{dataset_name} no skip:',\n",
    "        acc_str( out['num_accurate'], out['num_predictions']) )\n",
    "    return out\n",
    "\n",
    "def evaluate_all( limit: int = 1e5, topk: int = 1 ):\n",
    "    pile_out = evaluate( 'pile', limit, topk )\n",
    "    code_out = evaluate( 'code', limit, topk )\n",
    "    percentages = {\n",
    "        \"pile_skip\": 100*pile_out['num_skip_accurate']/pile_out['num_skip_predictions'],\n",
    "        \"pile\": 100*pile_out['num_accurate']/pile_out['num_predictions'],\n",
    "        \"code_skip\": 100*code_out['num_skip_accurate']/code_out['num_skip_predictions'],\n",
    "        \"code\": 100*code_out['num_accurate']/code_out['num_predictions'],\n",
    "    }\n",
    "    return percentages\n",
    "\n",
    "def save_numpy( array, name ):\n",
    "    filename = f'tmp/{opt.model_size}-{freq_multiple}x-{name}.npy'\n",
    "    os.makedirs( 'tmp', exist_ok=True )\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, np.array(array) )\n",
    "    print(\"saved successfully\")\n",
    "\n",
    "def delete_and_evaluate(\n",
    "        freq_multiple: float,\n",
    "        counter_sample_size: int = 5e4,\n",
    "        eval_sample_size: int = 1e5,\n",
    "        ):\n",
    "    # Count activation of MLP middle layers\n",
    "    pile_counters = count_keys( 'pile', sample_size=counter_sample_size, num_samples=1, check_accuracy=True )\n",
    "    code_counters = count_keys( 'code', sample_size=counter_sample_size, num_samples=1, check_accuracy=True )\n",
    "    \n",
    "    # Delete when the MLP layer activates way more for code than pile\n",
    "    ff_criterion = ( code_counters[0] > (freq_multiple*pile_counters[0]) )\n",
    "    sums = [ x.sum() for x in ff_criterion.detach().numpy() ]\n",
    "    num_removed = np.sum(sums)\n",
    "    print( \"%5d -\"%num_removed, sums )\n",
    "    opt.delete_ff_keys( ff_criterion )\n",
    "    \n",
    "    try:\n",
    "        # Save the indices that were deleted into the timestamped file\n",
    "        print(\"saving files...\")\n",
    "        now = datetime.datetime.now().strftime( \"%Y-%m-%d_%H:%M:%S\" )\n",
    "        save_numpy( ff_criterion,     f'criterion_{now}' )\n",
    "        save_numpy( pile_counters[0], f'counters-pile_{now}' )\n",
    "        save_numpy( code_counters[0], f'counters-code_{now}' )\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Did not save sadly :(\")\n",
    "\n",
    "    \n",
    "    # See the effect this has on performance\n",
    "    data = evaluate_all( eval_sample_size )\n",
    "    data['removed'] = num_removed\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_removals = []\n",
    "\n",
    "criteria = None\n",
    "\n",
    "for filename in pre_removals:\n",
    "    ff_criterion = np.load(filename)\n",
    "    if criteria is None:\n",
    "        criteria = np.zeros_like( ff_criterion )\n",
    "    criteria += ff_criterion\n",
    "\n",
    "    sums = [ x.sum() for x in ff_criterion ]\n",
    "    print( \"%5d -\"%np.sum(sums), sums )\n",
    "    #opt.delete_ff_keys( ff_criterion )\n",
    "\n",
    "criteria = 1 - criteria\n",
    "sums = [ x.sum() for x in criteria ]\n",
    "print( np.sum(sums), sums )\n",
    "opt.delete_ff_keys(  criteria )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Evaluate model before removal of any neurons\n",
    "data = evaluate_all( 1e5, topk=10 )\n",
    "df = df.append( data, ignore_index=True )\n",
    "print( df.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_MULTIPLE = 3.2\n",
    "\n",
    "for i in range(4):\n",
    "    print('\\n\\n- RUNNING RUN No', i )\n",
    "    data = delete_and_evaluate( FREQ_MULTIPLE )\n",
    "    df = df.append( data, ignore_index=True )\n",
    "    print( df.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('\\n\\n- RUNNING RUN No', i )\n",
    "    data = delete_and_evaluate( FREQ_MULTIPLE )\n",
    "    df = df.append( data, ignore_index=True )\n",
    "    print( df.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('\\n\\n- RUNNING RUN No', i )\n",
    "    data = delete_and_evaluate( FREQ_MULTIPLE )\n",
    "    df = df.append( data, ignore_index=True )\n",
    "    print( df.T )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
